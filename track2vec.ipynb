{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deezer playlist dataset et recomandandation de morceaux avec word2vec\n",
    "\n",
    "Dans ce mini projet nous allons mettre au point un réseau word2vec et nous en servir pour construire un outils de complétion de playlist (suggestion de morceaux). Les données sont hébergée sur le dépot suivant : http://github.com/comeetie/deezerplay.git. Pour en savoir plus sur word2vec et les données que nous allons utiliser vous pouvez lire les deux références sivantes :\n",
    "\n",
    "- Efficient estimation of word representations in vector space, Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. (https://arxiv.org/abs/1301.3781)\n",
    "- Word2vec applied to Recommendation: Hyperparameters Matter, H. Caselles-Dupré, F. Lesaint and J. Royo-Letelier. (https://arxiv.org/pdf/1804.04212.pdf)\n",
    "\n",
    "Les éléments que vous devez réaliser sont mis en évidence en <span style=\"color:red\">rouge</span> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des données\n",
    "Les données sont sous la forme d'une liste de playlist. Chaque playlist est elle me une liste avec l'identifiant deezer du morçeau suivi de l'identifiant de l'artiste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement des données de playlist\n",
    "import numpy as np\n",
    "data = np.load(\"./data/music_2.npy\",allow_pickle=True)\n",
    "[len(data), np.mean([len(p) for p in data])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données sur lequel nous allons trvailler contient 100000 playlist qui sont composeer d'en moyenne 24.1 morceaux. Nous allons commencer par ne conserver que les identifiants de morceau. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separation des ids de morçeau et d'artist\n",
    "playlist_track = [list(filter(lambda w: w.split(\"_\")[0]==u\"track\",playlist)) for playlist in data]\n",
    "playlist_artist = [list(filter(lambda w: w.split(\"_\")[0]==u\"artist\",playlist)) for playlist in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre de morceaux != ?\n",
    "tracks = np.unique(np.concatenate(playlist_track))\n",
    "Vt = len(tracks)\n",
    "Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_track[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre de morceaux différents dans ce data-set est assez élevé avec plus de 300 000 morceaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un dictionnnaire de morceau\n",
    "Nous allons affecter a chaque morceau un entier qui nous servira d'identifiant unique et d'entrée pour notre réseau. Pour économiser un peu nos ressources nous allons travailler dans ce TP que sur les morceaux qui apparaissent dans au moins deux playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre d'occurence de chaque morceaux ?\n",
    "track_counts = dict((tracks[i],0) for i in range(0, Vt))\n",
    "for p in playlist_track:\n",
    "    for a in p:\n",
    "        track_counts[a]=track_counts[a]+1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage des morceaux peu fréquent pour gangner un peu de temps au vue de nos ressource en temps de calcul  \n",
    "playlist_track_filter = [list(filter(lambda a : track_counts[a]> 1, playlist)) for playlist in playlist_track]\n",
    "# recupération des comptage\n",
    "counts  =  np.array(list(track_counts.values()))\n",
    "# trie\n",
    "order = np.argsort(-counts)\n",
    "# création de notre liste d'identifiant deezer\n",
    "tracks_list_ordered = np.array(list(track_counts.keys()))[order]\n",
    "# Taille de notre vocabulaire = nombre de morçeau conservés\n",
    "Vt=np.where(counts[order]==1)[0][0]\n",
    "# construction d'un dict id_morceaux id [0,Vt]\n",
    "track_dict = dict((tracks_list_ordered[i],i) for i in range(0, Vt))\n",
    "# conversion des playlist en liste d'entier\n",
    "corpus_num_track = [[track_dict[track] for track in play ] for play in playlist_track_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_num_track[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des ensembles d'apprentissage de test et de validation\n",
    "\n",
    "Pour apprendre les paramètre de notre méthode nous allons conserver les $l-1$ premiers morceaux de chaque playlist (avec $l$ la longueur de la playlist) pour l'apprentissage. Pour évaluer les performances de completion de notre méthode nous conservons pour chaque playlist les deux derniers morceaux. L'objectif sera de trouver le dernier a partir de l'avant dernier. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble de test et d'apprentissage\n",
    "index_tst = np.random.choice(100000,20000)\n",
    "index_val = np.setdiff1d(range(100000),index_tst)\n",
    "# le debut de chaque playlist est conservé pour l'apprentissage\n",
    "play_app  = [corpus_num_track[i][:(len(corpus_num_track[i])-1)] # On retire le dernier son de chaque playlist (en retirant également les playlists vide et les playlists avec un seul son)\n",
    "             for i in range(len(corpus_num_track)) if len(corpus_num_track[i])>1]\n",
    "# les deux derniers élemnts pour le test et la validation\n",
    "play_tst  = np.array([corpus_num_track[i][(len(corpus_num_track[i])-2):len(corpus_num_track[i])] \n",
    "             for i in index_tst if len(corpus_num_track[i])>3])\n",
    "play_val  = np.array([corpus_num_track[i][(len(corpus_num_track[i])-2):len(corpus_num_track[i])] \n",
    "             for i in index_val if len(corpus_num_track[i])>3])[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import de Keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense,Flatten\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyper-paramètres de word2vec :\n",
    "\n",
    "La méthode word2vec fait intervennir un certains nombre d'hyper paramètres. Nous allons les définirs et leurs donner des première valeurs que nous affinerons par la suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension de l'espace latent\n",
    "vector_dim = 30\n",
    "# taille de la fenêtre de voisinage\n",
    "window_width = 3\n",
    "# sur-échantillonage des exemples négatifs\n",
    "neg_sample = 5\n",
    "# taille des mini-batch\n",
    "min_batch_size = 50\n",
    "# coeff pour la loi de tirage des exemple negatif\n",
    "samp_coef = 0.5\n",
    "# coeff pour le subsampling\n",
    "sub_samp = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des tables de probabilité de tirage (lissée) et non lissée\n",
    "\n",
    "Pour tirer les exmples négatif nous avons besoin des fréquence lissé de chaque morceau dans notre dataset. De même pour sous échantilloner les morceaux très fréquents nous avons besoin des fréquence brutes. Nous allons calculer ces deux vecteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recupération des comptage\n",
    "counts = np.array(list(track_counts.values()),dtype='float')[order[:Vt]]\n",
    "# normalisation\n",
    "st =  counts/np.sum(counts)\n",
    "# lissage\n",
    "st_smooth = np.power(st,samp_coef)\n",
    "st_smooth = st_smooth/np.sum(st_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction du réseau word2vec\n",
    "\n",
    "Un réseau word2vec prend en entrée deux entiers correspondant à deux morceaux, ceux-ci sont plonger dans un espace latent de dimension (vector_dim) grâce a une couche de type embedding (vous devrez utilisez la même couche pour projeter les deux morceaux). Une fois ces deux vecteurs extraits le réseau doit calculer leur produit scalaire normaliser appleler cosine distance : \n",
    "\n",
    "$$cos(\\theta_{ij})=\\frac{z_i.z_j}{||z_i||||z_j||}$$ \n",
    "\n",
    "Pour réaliser ce traitement vous utiliserez une couche \"Dot\" pour \"dot product\". Le modèle utilise ensuite une couche de type sigmoid pour produire la sortie. Cette sortie vaudra 0 lorsque les deux morceaux sont des morceaux tirés aléatoirement dans l'ensemble du jeu de donnée et 1 lorsqu'il aurront était extraits de la même playslist. <span style=\"color:red\">A vous de créer le modèle keras Track2Vec correspondant à cette architecture.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrée deux entier (couple de morceaux)\n",
    "input_target = Input((1,), dtype='int32')\n",
    "input_context = Input((1,), dtype='int32')\n",
    "\n",
    "embedding = Embedding(Vt, vector_dim)\n",
    "zi = embedding(input_target)\n",
    "zj = embedding(input_context)\n",
    "\n",
    "dot_product = (Dot(2)([zi, zj]))\n",
    "flat = Flatten()(dot_product)\n",
    "output = Dense(1, activation='sigmoid',name=\"classif\")(flat)\n",
    "\n",
    "# definition du modèle\n",
    "Track2Vec = Model(inputs=[input_target, input_context], outputs=output)\n",
    "Track2Vec.compile(loss='binary_crossentropy', optimizer='adam',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Track2Vec.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du générateur de données\n",
    "\n",
    "Pour apprendre la couche de projection au coeur de notre modèle nous allons construire une générateur d'exemples positifs et négatifs de pair de morceaux proche ou aléatoires issues de nos données d'entrainement. La fonction suivante va permettre de générer de tels exemples a partir d'une playlist (seq) fournies en entrées. Cette fonction va tout d'abord construire tout les couples de morceau pouvant être extraient de la séquences s'ils se situent à moins de (windows) disance l'un de l'autres. Ces paires constitueront les paires positives. Les paires concernant deux morceaux très fréquents seront supprimer avec une probabilité qui dépendra de leur fréquences. Enfin un nombre d'exemple négatifs (correpondant neg_samples * nombre d'exemple positif) vont être tirés aléatoirement en utilisant la table de tirage (neg_sampling_table). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction générant les données associé a une séquence\n",
    "# seq : séquence d'entrée\n",
    "# neg_samples : nombre d'exemple négatif générés par example positif\n",
    "# neg_sampling_table : probabilité de tirage des exemples négatif\n",
    "# sub sampling_table : probabilité servant a sous échantilloner\n",
    "# sub_t : paramètre de sous échantillonage\n",
    "def word2vecSampling(seq,window,neg_samples,neg_sampling_table,sub_sampling_table,sub_t):\n",
    "    # taille du vocabulaire\n",
    "    V = len(neg_sampling_table)\n",
    "    # créations des paires positives a partir de la séquence\n",
    "    positives = skipgrams(sequence=seq, vocabulary_size=V, window_size=window,negative_samples=0)\n",
    "    ppairs    = np.array(positives[0])\n",
    "    # sous échantillonage\n",
    "    if (ppairs.shape[0]>0):\n",
    "        f = sub_sampling_table[ppairs[:,0]]\n",
    "        subprob = ((f-sub_t)/f)-np.sqrt(sub_t/f)\n",
    "        tokeep = (subprob<np.random.uniform(size=subprob.shape[0])) | (subprob<0)\n",
    "        ppairs = ppairs[tokeep,:]\n",
    "    nbneg     = ppairs.shape[0]*neg_samples\n",
    "    # tirage des paires négatives\n",
    "    if (nbneg > 0):\n",
    "        negex     = np.random.choice(V, nbneg, p=neg_sampling_table)\n",
    "        negexcontext = np.repeat(ppairs[:,0],neg_samples)\n",
    "        npairs    = np.transpose(np.stack([negexcontext,negex]))\n",
    "        pairs     = np.concatenate([ppairs,npairs],axis=0)\n",
    "        labels    = np.concatenate([np.repeat(1,ppairs.shape[0]),np.repeat(0,nbneg)])\n",
    "        perm      = np.random.permutation(len(labels))\n",
    "        res = [pairs[perm,:],labels[perm]]\n",
    "    else:\n",
    "        res=[[],[]]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Utilisez cette fonction pour constuire un générateur \"track_ns_generator\" de données qui va générer des exemples positifs et négatifs à partir de \"nbm\" playlists tirées aléatoirement dans le jeu de données \"corpus_num\" fournis en entrée. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition du générateur de couple de morceaux (y=0 <-> aléatoire, y=1 <-> proche dans une playlist)\n",
    "import random\n",
    "def track_ns_generator(corpus_num, nbm):\n",
    "    while 1:\n",
    "        # Tirage de `nbm` playlists aléatoires dans `corpus_num`\n",
    "        playlists = random.sample(corpus_num, nbm)\n",
    "        # Création des paires et de leur label\n",
    "        x1 = []\n",
    "        x2 = []\n",
    "        y = []\n",
    "        for playlist in playlists:\n",
    "            pairs, labels = word2vecSampling(playlist, window_width, neg_sample, st_smooth, st, sub_samp)\n",
    "            x1.extend(list(map(lambda pair: pair[0], pairs))) # Liste des avant-derniers éléments\n",
    "            x2.extend(list(map(lambda pair: pair[1], pairs))) # Liste des derniers éléments\n",
    "            y.extend(labels) # Liste des labels (0 = paires négatives; 1 = paires positives)\n",
    "\n",
    "        # Conversion en Array numpy pour la fonction d'entrainement\n",
    "        x1 = np.array(x1).reshape(len(x1))\n",
    "        x2 = np.array(x2).reshape(len(x2))\n",
    "        y = np.array(y).reshape(len(y))\n",
    "\n",
    "        yield [x1, x2], y"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apprentissage\n",
    "Vous devriez maintenant être en mesure d'apprendre votre premier modèle avec le code suivant. Cela devrait durer entre 15 et 30 min."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hist=Track2Vec.fit(track_ns_generator(play_app,min_batch_size),steps_per_epoch = 200,epochs=60)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sauvegarde de l'espace latent\n",
    "Nous pouvons une fois l'apprentissage effectué sauvegarder la position des morceaux dans l'espace latent avec le code suivant:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupérations des positions des morceaux dans l'espace de projection\n",
    "vectors_tracks = Track2Vec.get_weights()[0]\n",
    "with open('latent_positions.npy', 'wb') as f:\n",
    "    np.save(f, vectors_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et nous pouvons la recharger avec le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_tracks=np.load(\"./data/latent_positions.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation en complétion et évaluation\n",
    "Nous pouvons maintenant nous servir de cet espace pour faire des suggestions. <span style=\"color:red\">Construisez une fonction predict_batch qui prend en entrée un vecteur de numéro de morceaux (seeds), (s) un nombre de proposition a faire, les vecteurs des morceaux dans l'espace latent X et un kd-tree permettant d'accélérer les calculs de plus proche voisins. Pour faire ses propisitions cette fonctions retournera les indices des s plus proche voisins de chaque seeds. </span> Pour que ces predictions ne prennent pas trop de temps vous vous servirez d'un kd-tree (disponnible dans scikit learn) pour accélrer la recherche des plus proche voisins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "kdt = KDTree(vectors_tracks, leaf_size=10, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(seeds,k,X,kdt):\n",
    "    nearest_neighbors = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        dist, ind = kdt.query([X[seed]], k)\n",
    "        nearest_neighbors.append(ind)\n",
    "\n",
    "    return nearest_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Utilisez cette fonction pour proposer des morceaux pour compléter les playlist du jeu de données de validation (les seeds correspondent à la première colone de play_val).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = predict_batch(play_val[:,0],10,vectors_tracks,kdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Comparez ces suggestions avec la seconde colonne de play_val (les morceaux effectivement présents). Pour cela vous calculerez le hit@10 qui vaut 1 si le morceau effectivement présent dans la playlist fait partie des 10 propositions (ce score étant moyenné sur l'ensemble de validation) et le NDCG@10 (Normalized Discounted Cumulative Gain) qui prend en compte l'ordre des propositions. Ce second score vaut $1/log2(k+1)$ si la proposition k (k entre 1 et 10) est la proposition correcte et 0 si aucne proposition n'est correcte. Comme précedement vous calulerez le score moyen sur l'ensemble de validation. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndgcAtK(tracks_set, predictions_set):\n",
    "    hits = map(lambda track, predictions:\n",
    "               (1 / np.log2(np.where(predictions == track)[1][0] + 2)) if track in predictions else 0,\n",
    "               tracks_set, predictions_set)\n",
    "\n",
    "    return np.mean(list(hits))\n",
    "\n",
    "ndgcAtK(play_val[:,1], indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def hitAtK(tracks_set, predictions_set):\n",
    "    hits = map(lambda track, predictions:\n",
    "               1 if track in predictions else 0,\n",
    "               tracks_set, predictions_set)\n",
    "\n",
    "    return np.mean(list(hits))\n",
    "\n",
    "hitAtK(play_val[:,1], indexes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning des hyper paramètres\n",
    "\n",
    "<span style=\"color:red\">Vous pouvez maintenant essayer de faire varier les hyper paramètres pour améliorer vos performances. Attention au temps de calcul préparez une grille avec une dizaine de configurations différentes et évaluez chacune d'entre elles sur votre ensemblede validation.\n",
    "Evaluez les performances finales de la meilleure configuration trouvée sur l'ensemble de test. N'oubliez pas de sauvergader vos résultats.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus, un peu de musique\n",
    "\n",
    "Le fichier TrackArtists contient des méta.données sur les morceaux et les artiste pour un sous ensemble des 300000 morceaux présent dans le dataset. Nous pouvons nous en servir pour recherchez le numéro d'un morceau a partir de son titre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tr_meta=pd.read_csv(\"./TracksArtists.csv\")\n",
    "joindf = pd.DataFrame({\"track_id\":tracks_list_ordered[:Vt],\"index\":range(Vt)})\n",
    "meta = tr_meta.merge(joindf, left_on=\"track_id\",right_on=\"track_id\")\n",
    "meta.set_index(\"index\",inplace=True)\n",
    "meta[[\"title\",\"name\",\"preview\",\"track_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_track(title):\n",
    "    return meta.loc[meta[\"title\"]==title,:].index[0]\n",
    "\n",
    "tr=find_track(\"Hexagone\")\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radio\n",
    "\n",
    "L'api de deeezer permet de récupérer des informations sur les morceaux du dataset a partit de leur id deezer. Parmis ces infos lorsqu'elle est disponnible une url d'écoute d'un extrait gratuit est fournies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json \n",
    "def gettrackinfo(number):\n",
    "    track_url =  \"https://api.deezer.com/track/{}\".format(tracks_list_ordered[number].split(\"_\")[1])\n",
    "    with urllib.request.urlopen(track_url) as url:\n",
    "        data = json.loads(url.read().decode())\n",
    "    return data\n",
    "track_apidata = gettrackinfo(find_track(\"Hexagone\"))\n",
    "track_apidata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons donc nous en servir pour écouter un extrait :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio, clear_output\n",
    "display(Audio(track_apidata[\"preview\"],autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Créez une fonction radio qui prend en entrée un numéro de morceau dans le dataset et lance une serie de nb_track morceaux en tirant aléatoirement dans le voisinage du morceau courant le morceau suivant a écouter. La taille du voisinage sera paramétrable et vous supprimerez des propositions les morceaux déjà écouté. Vous traiterez les exceptions si le morceau ne dispose pas d'extrait disponnible. Vous povez supprimer le morceau courant avec la fonction clear_display.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def start_radio(seed,nb_candidates,duration,nbsteps=20):\n",
    "    print(meta.loc[seed,\"title\"])\n",
    "    display(Audio(meta.loc[seed,\"preview\"],autoplay=True))\n",
    "    time.sleep(duration)\n",
    "    clear_output()\n",
    "    already_played = [seed]\n",
    "    for i in range(nbsteps):\n",
    "        try:\n",
    "            # TODO\n",
    "        except:\n",
    "            print(\"track not found\")\n",
    "            pass\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_radio(find_track(\"Hexagone\"),5,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}